---
title: "Final Project Milestone - Dataset Evaluation"
author: "Michael Poff"
engine: knitr
execute:
  echo: true
  error: true
knitr:
  opts_chunk: 
    message: false
format:
  html:
    embed-resources: true
    code-tools: true
    code-fold: true
---

```{r setup}
#| context: setup
#| include: false
knitr::opts_chunk$set(echo = TRUE,error = FALSE, message = FALSE)
library(tidyverse)
library(tidymodels)
library(knitr)
library(wordcloud)
library(tidytext)
library(glue)
library(RColorBrewer)
library(kableExtra)
```

## Data

```{r data-import}

data <- read.csv('C:/Users/poffm/OneDrive/School/Calvin/INFO 602/FinalProject/data/article_data.csv')

```

The dataset contains `r nrow(data)` data points, each of which represents an article scraped from a number of information security news and intelligence providers (listed below). Each row contains the following features:

-   Title: The title of the article.
-   Content: The article contents.
-   Category: A manually labeled category label, chosen from one of nine options (listed below).
-   Zero-Shot Category: A label applied by the [HuggingFace zero-shot classification transformer](https://huggingface.co/tasks/zero-shot-classification) using the same set of nine options.
-   Zero-Shot Match: Indicates whether the zero-shot classification corresponds with the manual category assignment, expressed as TRUE or FALSE.

**Categories:**

-   Vulnerability Reports: Specific vulnerabilities found in software or hardware.
-   Security Updates and Patches: Latest releases of security patches and updates.
-   Emerging Threats and Attack Techniques: New cyber threats and attack methods.
-   Regulatory Changes and Compliance: Updates to cybersecurity laws and regulations.
-   Data Breaches and Security Incidents: Reports and analyses of recent data breaches or security incidents.
-   News and Industry Updates: General security news.
-   Marketing: Content primarily meant to highlight, advertise, or market a security product.

**Sources:**

-   [The Hacker News](https://thehackernews.com/)
-   [Bleeping Computer](https://www.bleepingcomputer.com/)
-   [Dark Reading](https://www.darkreading.com/)
-   [CISA Advisories](https://www.cisa.gov/news-events/cybersecurity-advisories)
-   [The Register - Security](https://www.theregister.com/security/)

The title and article data were scraped using the Python library [Trafilatura](https://github.com/adbar/trafilatura) which does a good job of scraping content and metadata from article web pages, however some results are skewed. Rather than manually refine the data, the model should learn to ignore this extraneous text.

## Exploratory Data Analysis

First, a quick look at a row of data. This is limited to a single row because the contents are quite large.
```{r quick-look}
kable(head(data, n=1))
```

#### Categories
This is a simple count of how many articles there are per category.
```{r summarize-categories}
data %>%
  select(category) %>%
  group_by(category) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>%
  kable("html", caption = "Number of Articles per Category") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

The low count of regulatory change articles may mean I need to remove the category altogether, or pull more articles manually to fill out that category and provide enough data for training and testing.

#### Zero Shot vs Manual Classification
```{r dare-to-compare}

data %>%
  group_by(zero.shot.match) %>%
  summarize(n = n()) %>%
  ggplot(aes(x = zero.shot.match, y = n, fill = zero.shot.match)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = n), vjust = -0.3) +
  scale_fill_manual(values = c("TRUE" = "steelblue", "FALSE" = "salmon")) +
  labs(title = "Zero-Shot Match Distribution",
       x = "Zero-Shot Match",
       y = "Number of Articles") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(face = "bold", hjust = 0.5))

```
This plot demonstrates that zero-shot classification correctly classifies articles 42% of the time. This is a good benchmark for our model to compare against a deep neural network with a generalized task.


#### Word Clouds
These word clouds distill the core themes across different cybersecurity categories, highlighting key terms. They offer a quick visual summary of the most discussed topics in the field.

```{r word-cloud}
data_clean <- data %>%
  # Break down each datapoint in content into individual words.
  unnest_tokens(word, content) %>%
  # Remove stop words from the tokenized data.
  anti_join(stop_words, by = "word") %>%
  mutate(word = str_remove_all(word, "[[:punct:]]"), # Removes punctuation
         word = str_remove_all(word, "[^A-Za-z]")) %>% # Removes non-standard characters, keeps only word characters and spaces
  filter(word != "") %>%
  select(category, word)

word_frequencies_by_category <- data_clean %>%
  group_by(`category`, word) %>%
  summarise(n = n(), .groups = 'drop') %>%
  arrange(`category`, desc(n))

# Loop through each unique category to create a word cloud
unique_categories <- unique(word_frequencies_by_category$category)

for(category in unique_categories) {
  cat_data <- filter(word_frequencies_by_category, category == !!category)
  
  # Display category name
  cat("\nWord Cloud for Category:", category, "\n")
  
  # Generate the word cloud for the current category
  wordcloud(words = cat_data$word, 
            freq = cat_data$n, 
            min.freq = 1,
            max.words = 100, 
            random.order = FALSE, 
            colors = brewer.pal(8, "Dark2"),
            scale = c(3, 0.5))  # Adjust scale for better visualization if needed
}

```

